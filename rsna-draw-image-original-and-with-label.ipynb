{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-06-14T19:25:20.439181Z","iopub.status.busy":"2024-06-14T19:25:20.438789Z","iopub.status.idle":"2024-06-14T19:25:20.446935Z","shell.execute_reply":"2024-06-14T19:25:20.445240Z","shell.execute_reply.started":"2024-06-14T19:25:20.439152Z"},"trusted":true},"outputs":[],"source":["import os\n","import glob\n","import pandas as pd\n","import numpy as np\n","import polars as pl\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import pydicom"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-06-14T19:41:40.215092Z","iopub.status.busy":"2024-06-14T19:41:40.214623Z","iopub.status.idle":"2024-06-14T19:41:40.252262Z","shell.execute_reply":"2024-06-14T19:41:40.250052Z","shell.execute_reply.started":"2024-06-14T19:41:40.215052Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["shape: (5, 26)\n","┌──────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬───────────┐\n","│ study_id ┆ spinal_ca ┆ spinal_ca ┆ spinal_ca ┆ … ┆ right_sub ┆ right_sub ┆ right_sub ┆ right_sub │\n","│ ---      ┆ nal_steno ┆ nal_steno ┆ nal_steno ┆   ┆ articular ┆ articular ┆ articular ┆ articular │\n","│ i64      ┆ sis_l1_l2 ┆ sis_l2_l3 ┆ sis_l3_l4 ┆   ┆ _stenosis ┆ _stenosis ┆ _stenosis ┆ _stenosis │\n","│          ┆ ---       ┆ ---       ┆ ---       ┆   ┆ _l2…      ┆ _l3…      ┆ _l4…      ┆ _l5…      │\n","│          ┆ str       ┆ str       ┆ str       ┆   ┆ ---       ┆ ---       ┆ ---       ┆ ---       │\n","│          ┆           ┆           ┆           ┆   ┆ str       ┆ str       ┆ str       ┆ str       │\n","╞══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪═══════════╡\n","│ 4003253  ┆ Normal/Mi ┆ Normal/Mi ┆ Normal/Mi ┆ … ┆ Normal/Mi ┆ Normal/Mi ┆ Normal/Mi ┆ Normal/Mi │\n","│          ┆ ld        ┆ ld        ┆ ld        ┆   ┆ ld        ┆ ld        ┆ ld        ┆ ld        │\n","│ 4646740  ┆ Normal/Mi ┆ Normal/Mi ┆ Moderate  ┆ … ┆ Moderate  ┆ Moderate  ┆ Moderate  ┆ Normal/Mi │\n","│          ┆ ld        ┆ ld        ┆           ┆   ┆           ┆           ┆           ┆ ld        │\n","│ 7143189  ┆ Normal/Mi ┆ Normal/Mi ┆ Normal/Mi ┆ … ┆ Normal/Mi ┆ Normal/Mi ┆ Normal/Mi ┆ Normal/Mi │\n","│          ┆ ld        ┆ ld        ┆ ld        ┆   ┆ ld        ┆ ld        ┆ ld        ┆ ld        │\n","│ 8785691  ┆ Normal/Mi ┆ Normal/Mi ┆ Normal/Mi ┆ … ┆ Normal/Mi ┆ Normal/Mi ┆ Normal/Mi ┆ Normal/Mi │\n","│          ┆ ld        ┆ ld        ┆ ld        ┆   ┆ ld        ┆ ld        ┆ ld        ┆ ld        │\n","│ 10728036 ┆ Normal/Mi ┆ Normal/Mi ┆ Normal/Mi ┆ … ┆ Normal/Mi ┆ Normal/Mi ┆ Moderate  ┆ Normal/Mi │\n","│          ┆ ld        ┆ ld        ┆ ld        ┆   ┆ ld        ┆ ld        ┆           ┆ ld        │\n","└──────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴───────────┘\n","shape: (1, 7)\n","┌──────────┬───────────┬─────────────────┬───────────────────────┬───────┬────────────┬────────────┐\n","│ study_id ┆ series_id ┆ instance_number ┆ condition             ┆ level ┆ x          ┆ y          │\n","│ ---      ┆ ---       ┆ ---             ┆ ---                   ┆ ---   ┆ ---        ┆ ---        │\n","│ i64      ┆ i64       ┆ i64             ┆ str                   ┆ str   ┆ f64        ┆ f64        │\n","╞══════════╪═══════════╪═════════════════╪═══════════════════════╪═══════╪════════════╪════════════╡\n","│ 4003253  ┆ 702807833 ┆ 8               ┆ Spinal Canal Stenosis ┆ L2/L3 ┆ 320.571429 ┆ 295.714286 │\n","└──────────┴───────────┴─────────────────┴───────────────────────┴───────┴────────────┴────────────┘\n","shape: (6_294, 3)\n","┌────────────┬────────────┬────────────────────┐\n","│ study_id   ┆ series_id  ┆ series_description │\n","│ ---        ┆ ---        ┆ ---                │\n","│ i64        ┆ i64        ┆ str                │\n","╞════════════╪════════════╪════════════════════╡\n","│ 4003253    ┆ 702807833  ┆ Sagittal T2/STIR   │\n","│ 4003253    ┆ 1054713880 ┆ Sagittal T1        │\n","│ 4003253    ┆ 2448190387 ┆ Axial T2           │\n","│ 4646740    ┆ 3201256954 ┆ Axial T2           │\n","│ 4646740    ┆ 3486248476 ┆ Sagittal T1        │\n","│ …          ┆ …          ┆ …                  │\n","│ 4287160193 ┆ 1507070277 ┆ Sagittal T2/STIR   │\n","│ 4287160193 ┆ 1820446240 ┆ Axial T2           │\n","│ 4290709089 ┆ 3274612423 ┆ Sagittal T2/STIR   │\n","│ 4290709089 ┆ 3390218084 ┆ Axial T2           │\n","│ 4290709089 ┆ 4237840455 ┆ Sagittal T1        │\n","└────────────┴────────────┴────────────────────┘\n"]}],"source":["# read data\n","INPUT_DIR = 'rsna-2024-lumbar-spine-degenerative-classification'\n","\n","train = pl.read_csv(f'{INPUT_DIR}/train.csv')\n","print(train.head())\n","train_label = pl.read_csv(f'{INPUT_DIR}/train_label_coordinates.csv')\n","print(train_label[1])\n","train_desc = pl.read_csv(f'{INPUT_DIR}/train_series_descriptions.csv')\n","print(train_desc)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-14T19:55:06.075599Z","iopub.status.busy":"2024-06-14T19:55:06.075031Z","iopub.status.idle":"2024-06-14T19:55:06.094513Z","shell.execute_reply":"2024-06-14T19:55:06.092482Z","shell.execute_reply.started":"2024-06-14T19:55:06.075558Z"},"trusted":true},"outputs":[],"source":["def graph_plot(study_id, series_id):\n","    train_label_combinations = pl.DataFrame()\n","    for row in train_label.iter_rows():\n","        if row[0]==study_id:\n","            print(pl.DataFrame(row[:3]).transpose())\n","            data = pl.DataFrame(row[:3]).transpose()\n","            train_label_combinations=pl.concat([train_label_combinations, data])\n","    print(train_label_combinations)\n","    \n","    #rename columns\n","    train_label_combinations = train_label_combinations.rename({\"column_0\":\"study_id\", \"column_1\":\"series_id\", \"column_2\":\"instance_number\"})\n","    #extract unique combination\n","    train_label_combinations = train_label_combinations.unique(subset=[\"study_id\", \"series_id\", \"instance_number\"]).sort([\"study_id\", \"series_id\", \"instance_number\"])\n","    \n","    instance_number_list = train_label_combinations.filter((pl.col(\"study_id\")==study_id) & (pl.col(\"series_id\")==series_id)).get_column(\"instance_number\")\n","    #instance_number_list\n","\n","    for instance_number in instance_number_list:\n","        #print(instance_number)\n","        print(f\"=====study_id:{study_id}, series_id:{series_id}, instance_number:{instance_number}=====\")\n","        #read image\n","        ds = pydicom.read_file(f'{INPUT_DIR}/train_images/{study_id}/{series_id}/{instance_number}.dcm')\n","        #draw original image\n","        df_plt = train_label.filter(\n","            (pl.col('study_id')==study_id)\n","            &(pl.col('series_id')==series_id)\n","            &(pl.col('instance_number')==instance_number)\n","        )\n","        plt.subplot(1,2,1)\n","        plt.imshow(ds.pixel_array, cmap='bone')\n","        #plt.title(f\"study_id:{study_id}, series_id:{series_id}, instance_number:{instance_number}\")\n","\n","        #draw original image + label\n","        #draw image\n","        df_plt = train_label.filter(\n","            (pl.col('study_id')==study_id)\n","            &(pl.col('series_id')==series_id)\n","            &(pl.col('instance_number')==instance_number)\n","        )\n","        plt.subplot(1,2,2)\n","        plt.imshow(ds.pixel_array, cmap='bone')\n","        #plt.title(f\"study_id:{study_id}, series_id:{series_id}, instance_number:{instance_number}\")\n","        #draw rabel\n","        for row in df_plt.iter_rows():\n","            plt.scatter(row[-2], row[-1], color='red')\n","        plt.show()\n","\n","study_id, series_id = 4290709089, 3274612423\n","graph_plot(study_id, series_id)"]},{"cell_type":"markdown","metadata":{},"source":["# Visualize the pixel array"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from PIL import Image\n","\n","def visualizeImage(directory):\n","    images = [f for f in os.listdir(directory) if f.endswith('.dcm')]\n","\n","    # Number of images\n","    grid_size = len(images)\n","\n","    grid_size = int(grid_size ** 0.5) + 1\n","\n","    fig, axes = plt.subplots(grid_size, grid_size, figsize=(15,15))\n","\n","    axes = axes.flatten()\n","\n","    for idx, file in enumerate(images):\n","        ds = pydicom.read_file(os.path.join(directory, file))\n","        axes[idx].imshow(ds.pixel_array, cmap='bone')\n","        axes[idx].set_title(file)\n","        axes[idx].axis('off')\n","\n","    # Hide any remaining empty subplots\n","    for i in range(idx + 1, len(axes)):\n","        axes[i].axis('off')\n","\n","    plt.tight_layout()\n","    plt.show\n","\n","directory =f'{INPUT_DIR}/train_images/{study_id}/{series_id}'\n","visualizeImage(directory)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def list_of_picture(directory):\n","    all_the_pic = []\n","    # path_dir = os.path.join(direc)\n","\n","    for i in os.listdir(directory):\n","        all_the_pic.append(i)\n","    return all_the_pic\n","\n","path_dir = f'{INPUT_DIR}/train_images/{study_id}/{series_id}'\n","list_of_picture(path_dir)"]},{"cell_type":"markdown","metadata":{},"source":["# Check for missing data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["data = pd.read_csv(\"rsna-2024-lumbar-spine-degenerative-classification/train.csv\")\n","missing_values_count = pd.isnull(data).sum()\n","print(missing_values_count)"]},{"cell_type":"markdown","metadata":{},"source":["# Checking the distribution of train dataset"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df = pd.read_csv(f\"{INPUT_DIR}/train.csv\")\n","\n","# Function for melting columns\n","def melting_columns(df):\n","    df_melted = pd.melt(df, \n","                            id_vars=['study_id'], \n","                            value_vars=[\n","                                'spinal_canal_stenosis_l1_l2', 'spinal_canal_stenosis_l2_l3', 'spinal_canal_stenosis_l3_l4', \n","                                'spinal_canal_stenosis_l4_l5', 'spinal_canal_stenosis_l5_s1', 'left_neural_foraminal_narrowing_l1_l2', \n","                                'left_neural_foraminal_narrowing_l2_l3', 'left_neural_foraminal_narrowing_l3_l4', \n","                                'left_neural_foraminal_narrowing_l4_l5', 'left_neural_foraminal_narrowing_l5_s1', \n","                                'right_neural_foraminal_narrowing_l1_l2', 'right_neural_foraminal_narrowing_l2_l3', \n","                                'right_neural_foraminal_narrowing_l3_l4', 'right_neural_foraminal_narrowing_l4_l5', \n","                                'right_neural_foraminal_narrowing_l5_s1', 'left_subarticular_stenosis_l1_l2', \n","                                'left_subarticular_stenosis_l2_l3', 'left_subarticular_stenosis_l3_l4', \n","                                'left_subarticular_stenosis_l4_l5', 'left_subarticular_stenosis_l5_s1', \n","                                'right_subarticular_stenosis_l1_l2', 'right_subarticular_stenosis_l2_l3', \n","                                'right_subarticular_stenosis_l3_l4', 'right_subarticular_stenosis_l4_l5', \n","                                'right_subarticular_stenosis_l5_s1'\n","                            ], \n","                            var_name='condition', \n","                            value_name='severity')\n","    return df_melted\n","\n","# Visualize the distribution\n","def distribution_graph(ax, df, title):\n","    df_melted = melting_columns(df) #From avobe functions\n","\n","    print(df_melted.head())\n","    print(len(df_melted))\n","\n","    # Check the distribution of severity levels\n","    severity_counts = df_melted['severity'].value_counts()\n","    print(severity_counts)\n","\n","    # Plot pie chart\n","    ax.pie(severity_counts, \n","           labels=severity_counts.index, \n","           autopct='%1.1f%%', \n","           startangle=90, \n","           colors=plt.get_cmap('Set2').colors)\n","    ax.set_title(title)\n","    \n","\n"]},{"cell_type":"markdown","metadata":{},"source":["# Percentage of distribution each severity on Original Data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fig, ax = plt.subplots(figsize=(10, 8))\n","\n","distribution_graph(ax, df, \"Distrbution on original dataset\")"]},{"cell_type":"markdown","metadata":{},"source":["# Impute data as most recent value using Skit Learn on train.csv"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.impute import SimpleImputer\n","\n","df_copy = df.copy()\n","\n","categorical_columns = df_copy.select_dtypes(include=['object']).columns\n","\n","categorical_imputer = SimpleImputer(strategy='most_frequent')\n","df_copy[categorical_columns] = categorical_imputer.fit_transform(df_copy[categorical_columns])\n","\n","# Checking missing data again\n","print(df_copy.isnull().sum())\n","\n","\n","df_melted_copy = melting_columns(df_copy)\n"]},{"cell_type":"markdown","metadata":{},"source":["# Compare the before and after imputation"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(15,10))\n","\n","distribution_graph(axes[0], df, \"Distribution Graph on Original Dataset\")\n","distribution_graph(axes[1], df_copy, \"Distribution Graph on Imputed Dataset\")\n","\n","# Display the plots\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["# Creating Custom Dataset from given image data and coordinates"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Label string: Moderate\n","Label string: Normal/Mild\n","Label string: Normal/Mild\n","Label string: Normal/Mild\n","Label string: Severe\n"]},{"ename":"RuntimeError","evalue":"stack expects each tensor to be equal size, but got [320, 320] at entry 0 and [512, 512] at entry 1","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[4], line 94\u001b[0m\n\u001b[1;32m     87\u001b[0m dataset \u001b[38;5;241m=\u001b[39m SpinalDataset(root_dir\u001b[38;5;241m=\u001b[39mINPUT_DIR, \n\u001b[1;32m     88\u001b[0m                         coordinates_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mINPUT_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/train_label_coordinates.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m     89\u001b[0m                         train\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_images\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     90\u001b[0m                         train_data\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mINPUT_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/train.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     92\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m DataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 94\u001b[0m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mImages shape:\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLabels:\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/Desktop/College/research_lumber_spine/venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[0;32m~/Desktop/College/research_lumber_spine/venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n","File \u001b[0;32m~/Desktop/College/research_lumber_spine/venv/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/Desktop/College/research_lumber_spine/venv/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py:317\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[1;32m    257\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[1;32m    259\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 317\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/Desktop/College/research_lumber_spine/venv/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py:174\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    171\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m--> 174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n","File \u001b[0;32m~/Desktop/College/research_lumber_spine/venv/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py:142\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m--> 142\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m    145\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n","File \u001b[0;32m~/Desktop/College/research_lumber_spine/venv/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py:223\u001b[0m, in \u001b[0;36mcollate_numpy_array_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np_str_obj_array_pattern\u001b[38;5;241m.\u001b[39msearch(elem\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mstr) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(default_collate_err_msg_format\u001b[38;5;241m.\u001b[39mformat(elem\u001b[38;5;241m.\u001b[39mdtype))\n\u001b[0;32m--> 223\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/Desktop/College/research_lumber_spine/venv/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py:142\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m--> 142\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m    145\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n","File \u001b[0;32m~/Desktop/College/research_lumber_spine/venv/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py:214\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    212\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    213\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[0;32m--> 214\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [320, 320] at entry 0 and [512, 512] at entry 1"]}],"source":["import os\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","import pydicom\n","import numpy as np\n","from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n","\n","def structure_for_train_csv(condition):\n","        condition = condition.lower().replace(' ', '_').replace('/', '_')\n","        return condition\n","\n","\n","class SpinalDataset(Dataset):\n","    def __init__(self, root_dir, coordinates_file, train, train_data, transform=None): # Setup the necessary attributes\n","        self.root_dir = root_dir\n","        self.coordinates= pd.read_csv(coordinates_file)\n","        self.train_data = pd.read_csv(train_data)\n","        self.train = train\n","        self.transform = transform\n","\n","        # Define label encoder and one hot encoder\n","        self.label_encoder = LabelEncoder()\n","        self.onehot_encoder = OneHotEncoder(sparse_output=False)\n","\n","        # Fit the label encoder and one hot encoder\n","        conditions = [\"Normal/Mild\", \"Moderate\", \"Severe\"]\n","\n","        self.label_encoder.fit(conditions)\n","        integer_encoded = self.label_encoder.transform(conditions).reshape(-1, 1)\n","        self.onehot_encoder.fit(integer_encoded)\n","\n","        # Define sample weights\n","        self.weights = {\"Normal/Mild\": 1, \"Moderate\": 2, \"Severe\": 4}\n","\n","    def __len__(self): # Returns the length of the Dataframe. More specifically numbers of rows in the dataset\n","        return len(self.coordinates)\n","    \n","    def __getitem__(self, idx): # This method retrieves a single sample (images and label) from the dataset at the specified index (idx).\n","        row = self.coordinates.iloc[idx]\n","        study_id = row['study_id']\n","        series_id = row['series_id']\n","        instance = row['instance_number']\n","        condition = row['condition']\n","        level = row['level']\n","        x = row['x']\n","        y = row['y'] \n","\n","        # Construct the path to the DICOM\n","        dicom_file_path = os.path.join(self.root_dir, self.train, str(study_id), str(series_id), f\"{instance}.dcm\")\n","        \n","\n","        # Load the DICOM images\n","        images = self.load_dicom_image(dicom_file_path)\n","      \n","        \n","\n","        if self.transform:\n","            images = self.transform(images)\n","\n","         # Extract condition for the specified level\n","        condition_column = f'{condition}_{level}'\n","        condition_column = structure_for_train_csv(condition_column)\n","        label_str = self.train_data.loc[self.train_data['study_id'] == study_id, condition_column].values[0]\n","        # Encode the label\n","        label_encoded = self.label_encoder.transform([label_str])\n","        label_onehot = self.onehot_encoder.transform(label_encoded.reshape(-1, 1))\n","        label = torch.tensor(label_onehot, dtype=torch.float32).squeeze()\n","\n","\n","        print(f\"Label string: {label_str}\")\n","        # Calculate weight for the sample\n","        weight = self.weights.get(label_str, 1)  # Default to 1 if condition not found\n","\n","        return images, label, weight  \n","        \n","\n","        # Load the DICOM images\n","    def load_dicom_image(self, file_path):\n","        dicom = pydicom.dcmread(file_path)\n","        image = dicom.pixel_array\n","        normalize = image.astype(np.float32) / image.max()\n","        return normalize\n","\n","\n","# Example usage\n","dataset = SpinalDataset(root_dir=INPUT_DIR, \n","                        coordinates_file=f'{INPUT_DIR}/train_label_coordinates.csv', \n","                        train='train_images',\n","                        train_data=f'{INPUT_DIR}/train.csv')\n","\n","dataloader = DataLoader(dataset, batch_size=5, shuffle=True)\n","\n","for images, labels, weights in dataloader:\n","    print(\"Images shape:\", images.shape)\n","    print(\"Labels:\", labels)\n","    print(\"Weights:\", weights)\n","    break"]},{"cell_type":"markdown","metadata":{},"source":["# Testing OneHotEncoding"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np\n","from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n","\n","# Define the conditions\n","conditions = [\"Normal/Mild\", \"Moderate\", \"Severe\"]\n","\n","# Initialize and fit the LabelEncoder\n","label_encoder = LabelEncoder()\n","label_encoder.fit(conditions)\n","\n","# Convert labels to numerical values\n","integer_encoded = label_encoder.transform(conditions).reshape(-1, 1)\n","\n","# Initialize and fit the OneHotEncoder\n","onehot_encoder = OneHotEncoder(sparse_output=False)\n","onehot_encoder.fit(integer_encoded)\n","\n","# Example labels to encode\n","labels = [\"Normal/Mild\", \"Moderate\", \"Severe\"]\n","\n","# Convert to numerical labels\n","integer_encoded = label_encoder.transform(labels).reshape(-1, 1)\n","\n","# Convert to one-hot encoded vectors\n","onehot_encoded = onehot_encoder.transform(integer_encoded)\n","\n","print(\"Integer Encoded:\")\n","print(integer_encoded)\n","\n","print(\"One-Hot Encoded:\")\n","print(onehot_encoded)"]},{"cell_type":"markdown","metadata":{},"source":["# Find minimum Image size minwidth and minheight"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Average Width: 446, Average Height: 451\n"]}],"source":["# Path to image directory\n","image_dir = \"rsna-2024-lumbar-spine-degenerative-classification/train_images/\"\n","\n","def findingShape():\n","    # Initialize variable to store minimum width and minimum height\n","    # min_width, min_height = float('inf'), float('inf')\n","    total_height, total_width, num_images = 0, 0, 0\n","\n","    # Iterate through all images in the directoey\n","    for image_name in os.listdir(image_dir):\n","        study_dir = os.path.join(image_dir, image_name)\n","        if not os.path.isdir(study_dir):\n","            continue\n","\n","        for series_id in os.listdir(study_dir):\n","            series_dir = os.path.join(study_dir, series_id)\n","            if not os.path.isdir(series_dir):\n","                continue\n","\n","            for dicom_file in os.listdir(series_dir):\n","                dicom_path = os.path.join(series_dir, dicom_file)\n","\n","                try:\n","                    ds = pydicom.dcmread(dicom_path)\n","                    image_array = ds.pixel_array\n","                    \n","                    if len(image_array.shape) == 2:  # Grayscale size\n","                        height, width = image_array.shape\n","\n","                    elif len(image_array.shape) == 3:  # Color including \n","                        height, width, _ = image_array.shape\n","                        \n","                    else:\n","                        raise ValueError(f\"Unexpected image shape: {image_array.shape}\")\n","\n","                    # Accumulate total size\n","                    total_width += width\n","                    total_height += height\n","                    num_images += 1\n","                except Exception as e:\n","                    print(f\"Failed to process {dicom_path}: {e}\")\n","\n","\n","\n","    # Calculate average \n","    if num_images > 0:\n","        avg_width = total_width // num_images\n","        avg_height = total_height // num_images\n","        print(f\"Average Width: {avg_width}, Average Height: {avg_height}\")\n","    else: \n","        print(\"No images\")\n","\n","findingShape()"]},{"cell_type":"markdown","metadata":{},"source":["# Normalize the images using "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["total_height, total_width, num_images = 0, 0, 0\n","\n","# Iterate through all images in the directoey\n","for image_name in os.listdir(image_dir):\n","    study_dir = os.path.join(image_dir, image_name)\n","    if not os.path.isdir(study_dir):\n","        continue\n","\n","    for series_id in os.listdir(study_dir):\n","        series_dir = os.path.join(study_dir, series_id)\n","        if not os.path.isdir(series_dir):\n","            continue\n","\n","        for dicom_file in os.listdir(series_dir):\n","            dicom_path = os.path.join(series_dir, dicom_file)\n"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"databundleVersionId":8561470,"sourceId":71549,"sourceType":"competition"}],"dockerImageVersionId":30732,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.4"}},"nbformat":4,"nbformat_minor":4}
