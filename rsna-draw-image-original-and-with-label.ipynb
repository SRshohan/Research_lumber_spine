{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-06-14T19:25:20.439181Z","iopub.status.busy":"2024-06-14T19:25:20.438789Z","iopub.status.idle":"2024-06-14T19:25:20.446935Z","shell.execute_reply":"2024-06-14T19:25:20.445240Z","shell.execute_reply.started":"2024-06-14T19:25:20.439152Z"},"trusted":true},"outputs":[],"source":["import os\n","import glob\n","import pandas as pd\n","import numpy as np\n","import polars as pl\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import pydicom"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-14T19:41:40.215092Z","iopub.status.busy":"2024-06-14T19:41:40.214623Z","iopub.status.idle":"2024-06-14T19:41:40.252262Z","shell.execute_reply":"2024-06-14T19:41:40.250052Z","shell.execute_reply.started":"2024-06-14T19:41:40.215052Z"},"trusted":true},"outputs":[],"source":["# read data\n","INPUT_DIR = 'rsna-2024-lumbar-spine-degenerative-classification'\n","\n","train = pl.read_csv(f'{INPUT_DIR}/train.csv')\n","print(train.head())\n","train_label = pl.read_csv(f'{INPUT_DIR}/train_label_coordinates.csv')\n","print(train_label[1])\n","train_desc = pl.read_csv(f'{INPUT_DIR}/train_series_descriptions.csv')\n","print(train_desc)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-14T19:55:06.075599Z","iopub.status.busy":"2024-06-14T19:55:06.075031Z","iopub.status.idle":"2024-06-14T19:55:06.094513Z","shell.execute_reply":"2024-06-14T19:55:06.092482Z","shell.execute_reply.started":"2024-06-14T19:55:06.075558Z"},"trusted":true},"outputs":[],"source":["def graph_plot(study_id, series_id):\n","    train_label_combinations = pl.DataFrame()\n","    for row in train_label.iter_rows():\n","        if row[0]==study_id:\n","            print(pl.DataFrame(row[:3]).transpose())\n","            data = pl.DataFrame(row[:3]).transpose()\n","            train_label_combinations=pl.concat([train_label_combinations, data])\n","    print(train_label_combinations)\n","    \n","    #rename columns\n","    train_label_combinations = train_label_combinations.rename({\"column_0\":\"study_id\", \"column_1\":\"series_id\", \"column_2\":\"instance_number\"})\n","    #extract unique combination\n","    train_label_combinations = train_label_combinations.unique(subset=[\"study_id\", \"series_id\", \"instance_number\"]).sort([\"study_id\", \"series_id\", \"instance_number\"])\n","    \n","    instance_number_list = train_label_combinations.filter((pl.col(\"study_id\")==study_id) & (pl.col(\"series_id\")==series_id)).get_column(\"instance_number\")\n","    #instance_number_list\n","\n","    for instance_number in instance_number_list:\n","        #print(instance_number)\n","        print(f\"=====study_id:{study_id}, series_id:{series_id}, instance_number:{instance_number}=====\")\n","        #read image\n","        ds = pydicom.read_file(f'{INPUT_DIR}/train_images/{study_id}/{series_id}/{instance_number}.dcm')\n","        #draw original image\n","        df_plt = train_label.filter(\n","            (pl.col('study_id')==study_id)\n","            &(pl.col('series_id')==series_id)\n","            &(pl.col('instance_number')==instance_number)\n","        )\n","        plt.subplot(1,2,1)\n","        plt.imshow(ds.pixel_array, cmap='bone')\n","        #plt.title(f\"study_id:{study_id}, series_id:{series_id}, instance_number:{instance_number}\")\n","\n","        #draw original image + label\n","        #draw image\n","        df_plt = train_label.filter(\n","            (pl.col('study_id')==study_id)\n","            &(pl.col('series_id')==series_id)\n","            &(pl.col('instance_number')==instance_number)\n","        )\n","        plt.subplot(1,2,2)\n","        plt.imshow(ds.pixel_array, cmap='bone')\n","        #plt.title(f\"study_id:{study_id}, series_id:{series_id}, instance_number:{instance_number}\")\n","        #draw rabel\n","        for row in df_plt.iter_rows():\n","            plt.scatter(row[-2], row[-1], color='red')\n","        plt.show()\n","\n","study_id, series_id = 4290709089, 3274612423\n","graph_plot(study_id, series_id)"]},{"cell_type":"markdown","metadata":{},"source":["# Visualize the pixel array"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from PIL import Image\n","\n","def visualizeImage(directory):\n","    images = [f for f in os.listdir(directory) if f.endswith('.dcm')]\n","\n","    # Number of images\n","    grid_size = len(images)\n","\n","    grid_size = int(grid_size ** 0.5) + 1\n","\n","    fig, axes = plt.subplots(grid_size, grid_size, figsize=(15,15))\n","\n","    axes = axes.flatten()\n","\n","    for idx, file in enumerate(images):\n","        ds = pydicom.read_file(os.path.join(directory, file))\n","        axes[idx].imshow(ds.pixel_array, cmap='bone')\n","        axes[idx].set_title(file)\n","        axes[idx].axis('off')\n","\n","    # Hide any remaining empty subplots\n","    for i in range(idx + 1, len(axes)):\n","        axes[i].axis('off')\n","\n","    plt.tight_layout()\n","    plt.show\n","\n","directory =f'{INPUT_DIR}/train_images/{study_id}/{series_id}'\n","visualizeImage(directory)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def list_of_picture(directory):\n","    all_the_pic = []\n","    # path_dir = os.path.join(direc)\n","\n","    for i in os.listdir(directory):\n","        all_the_pic.append(i)\n","    return all_the_pic\n","\n","path_dir = f'{INPUT_DIR}/train_images/{study_id}/{series_id}'\n","list_of_picture(path_dir)"]},{"cell_type":"markdown","metadata":{},"source":["# Check for missing data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["data = pd.read_csv(\"rsna-2024-lumbar-spine-degenerative-classification/train.csv\")\n","missing_values_count = pd.isnull(data).sum()\n","print(missing_values_count)"]},{"cell_type":"markdown","metadata":{},"source":["# Checking the distribution of train dataset"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df = pd.read_csv(f\"{INPUT_DIR}/train.csv\")\n","\n","# Function for melting columns\n","def melting_columns(df):\n","    df_melted = pd.melt(df, \n","                            id_vars=['study_id'], \n","                            value_vars=[\n","                                'spinal_canal_stenosis_l1_l2', 'spinal_canal_stenosis_l2_l3', 'spinal_canal_stenosis_l3_l4', \n","                                'spinal_canal_stenosis_l4_l5', 'spinal_canal_stenosis_l5_s1', 'left_neural_foraminal_narrowing_l1_l2', \n","                                'left_neural_foraminal_narrowing_l2_l3', 'left_neural_foraminal_narrowing_l3_l4', \n","                                'left_neural_foraminal_narrowing_l4_l5', 'left_neural_foraminal_narrowing_l5_s1', \n","                                'right_neural_foraminal_narrowing_l1_l2', 'right_neural_foraminal_narrowing_l2_l3', \n","                                'right_neural_foraminal_narrowing_l3_l4', 'right_neural_foraminal_narrowing_l4_l5', \n","                                'right_neural_foraminal_narrowing_l5_s1', 'left_subarticular_stenosis_l1_l2', \n","                                'left_subarticular_stenosis_l2_l3', 'left_subarticular_stenosis_l3_l4', \n","                                'left_subarticular_stenosis_l4_l5', 'left_subarticular_stenosis_l5_s1', \n","                                'right_subarticular_stenosis_l1_l2', 'right_subarticular_stenosis_l2_l3', \n","                                'right_subarticular_stenosis_l3_l4', 'right_subarticular_stenosis_l4_l5', \n","                                'right_subarticular_stenosis_l5_s1'\n","                            ], \n","                            var_name='condition', \n","                            value_name='severity')\n","    return df_melted\n","\n","# Visualize the distribution\n","def distribution_graph(ax, df, title):\n","    df_melted = melting_columns(df) #From avobe functions\n","\n","    print(df_melted.head())\n","    print(len(df_melted))\n","\n","    # Check the distribution of severity levels\n","    severity_counts = df_melted['severity'].value_counts()\n","    print(severity_counts)\n","\n","    # Plot pie chart\n","    ax.pie(severity_counts, \n","           labels=severity_counts.index, \n","           autopct='%1.1f%%', \n","           startangle=90, \n","           colors=plt.get_cmap('Set2').colors)\n","    ax.set_title(title)\n","    \n","\n"]},{"cell_type":"markdown","metadata":{},"source":["# Percentage of distribution each severity on Original Data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fig, ax = plt.subplots(figsize=(10, 8))\n","\n","distribution_graph(ax, df, \"Distrbution on original dataset\")"]},{"cell_type":"markdown","metadata":{},"source":["# Impute data as most recent value using Skit Learn on train.csv"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.impute import SimpleImputer\n","\n","df_copy = df.copy()\n","\n","categorical_columns = df_copy.select_dtypes(include=['object']).columns\n","\n","categorical_imputer = SimpleImputer(strategy='most_frequent')\n","df_copy[categorical_columns] = categorical_imputer.fit_transform(df_copy[categorical_columns])\n","\n","# Checking missing data again\n","print(df_copy.isnull().sum())\n","\n","\n","df_melted_copy = melting_columns(df_copy)\n","\n","percentage(df_melted_copy)"]},{"cell_type":"markdown","metadata":{},"source":["# Compare the before and after imputation"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(15,10))\n","\n","distribution_graph(axes[0], df, \"Distribution Graph on Original Dataset\")\n","distribution_graph(axes[1], df_copy, \"Distribution Graph on Imputed Dataset\")\n","\n","# Display the plots\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["# Creating Custom Dataset from given image data and coordinates"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import os\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","import pydicom\n","import numpy as np\n","from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n","\n","\n","class SpinalDataset(Dataset):\n","    def __init__(self, root_dir, coordinates_file, train, train_data, transform=None): # Setup the necessary attributes\n","        self.root_dir = root_dir\n","        self.coordinates= pd.read_csv(coordinates_file)\n","        self.train_data = pd.read_csv(train_data)\n","        self.train = train\n","        self.transform = transform\n","\n","        # Define label encoder and one hot encoder\n","        self.label_encoder = LabelEncoder()\n","        self.onehot_encoder = OneHotEncoder(sparse_output=False)\n","\n","        # Fit the label encoder and one hot encoder\n","        conditions = [\"Normal/Mild\", \"Moderate\", \"Severe\"]\n","\n","        self.label_encoder.fit(conditions)\n","        integer_encoded = self.label_encoder.transform(conditions).reshape(-1, 1)\n","        self.onehot_encoder.fit(integer_encoded)\n","\n","\n","        # Define sample weights\n","        self.weights = {\"Normal/Mild\": 1, \"Moderate\": 2, \"Severe\": 4}\n","\n","    def __len__(self): # Returns the length of the Dataframe. More specifically numbers of rows in the dataset\n","        return len(self.coordinates)\n","    \n","    def __getitem__(self, idx): # This method retrieves a single sample (images and label) from the dataset at the specified index (idx).\n","        row = self.coordinates.iloc[idx]\n","        study_id = row['study_id']\n","        series_id = row['series_id']\n","        instance = row['instance_number']\n","        condition = row['condition']\n","        level = row['level']\n","        x = row['x']\n","        y = row['y'] \n","\n","        # Construct the path to the DICOM\n","        series_path = os.path.join(self.root_dir, self.train, str(study_id), str(series_id), str(instance))\n","        dicom_files = [os.path.join(series_path, f) for f in os.listdir(series_path) if f.endswith('.dcm')]\n","\n","        # Load the DICOM images\n","        images = [self.load_dicom_image(f) for f in dicom_files]\n","        images = np.stack(images, axis=0)  # Stack images along a new dimension\n","\n","        if self.transform:\n","            images = self.transform(images)\n","\n","         # Extract condition for the specified level\n","        condition_column = f'{condition}_{level}'\n","        label_str = self.train_data.loc[self.train_data['study_id'] == study_id, condition_column].values[0]\n","        # Encode the label\n","        label_encoded = self.label_encoder.transform([label_str])\n","        label_onehot = self.onehot_encoder.transform(label_encoded.reshape(-1, 1))\n","        label = torch.tensor(label_onehot, dtype=torch.float32).squeeze()\n","\n","\n","       \n","        # Calculate weight for the sample\n","        weight = self.weights.get(label_str, 1)  # Default to 1 if condition not found\n","\n","        return images, label, weight  \n","        \n","\n","        # Load the DICOM images\n","    def load_dicom_image(self, file_path):\n","        dicom = pydicom.dcmread(file_path)\n","        image = dicom.pixel_array\n","        return image\n","\n","\n","# Example usage\n","dataset = SpinalDataset(root_dir=INPUT_DIR, \n","                        coordinates_file=f'{INPUT_DIR}/train_label_coordinates.csv', \n","                        train='train_images',\n","                        train_data=f'{INPUT_DIR}/train.csv')\n","\n","dataloader = DataLoader(dataset, batch_size=1, shuffle=False)\n","\n","for images, labels, weights in dataloader:\n","    print(\"Images shape:\", images.shape)\n","    print(\"Labels:\", labels)\n","    print(\"Weights:\", weights)\n","    break\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["# Testing OneHotEncoding"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np\n","from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n","\n","# Define the conditions\n","conditions = [\"Normal/Mild\", \"Moderate\", \"Severe\"]\n","\n","# Initialize and fit the LabelEncoder\n","label_encoder = LabelEncoder()\n","label_encoder.fit(conditions)\n","\n","# Convert labels to numerical values\n","integer_encoded = label_encoder.transform(conditions).reshape(-1, 1)\n","\n","# Initialize and fit the OneHotEncoder\n","onehot_encoder = OneHotEncoder(sparse_output=False)\n","onehot_encoder.fit(integer_encoded)\n","\n","# Example labels to encode\n","labels = [\"Normal/Mild\", \"Moderate\", \"Severe\"]\n","\n","# Convert to numerical labels\n","integer_encoded = label_encoder.transform(labels).reshape(-1, 1)\n","\n","# Convert to one-hot encoded vectors\n","onehot_encoded = onehot_encoder.transform(integer_encoded)\n","\n","print(\"Integer Encoded:\")\n","print(integer_encoded)\n","\n","print(\"One-Hot Encoded:\")\n","print(onehot_encoded)\n","\n","\n"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"databundleVersionId":8561470,"sourceId":71549,"sourceType":"competition"}],"dockerImageVersionId":30732,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.4"}},"nbformat":4,"nbformat_minor":4}
